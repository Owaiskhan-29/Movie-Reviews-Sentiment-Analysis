{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Reviews Sentiment Analysis (NLP Project)\n",
    "This notebook demonstrates how to perform sentiment analysis on IMDB movie reviews.\n",
    "We will go through the following steps:\n",
    "- Load and explore the dataset\n",
    "- Preprocess the text data (cleaning, removing noise, stemming)\n",
    "- Convert text into numerical features (Bag of Words)\n",
    "- Train Naive Bayes classifiers\n",
    "- Evaluate the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "We start by importing the necessary Python libraries for data processing, NLP, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Exploring the Dataset\n",
    "Let's load the IMDB dataset and check its shape and the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('IMDB.csv')\n",
    "dataset.shape  # Checking shape (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()  # Displaying first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentiment'].value_counts()  # Checking class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding Target Labels\n",
    "Convert the sentiment labels from strings to binary (positive=1, negative=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentiment'].replace({'positive': 1, 'negative': 0}, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning\n",
    "We will clean the text in the following steps:\n",
    "1. Remove HTML tags\n",
    "2. Remove special characters\n",
    "3. Convert to lowercase\n",
    "4. Remove stopwords\n",
    "5. Perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "def clean(text):\n",
    "    return re.sub(re.compile(r'<.*?>'), '', text)\n",
    "dataset['review'] = dataset['review'].apply(clean)\n",
    "dataset['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "def is_special(text):\n",
    "    return ''.join([ch if ch.isalnum() else ' ' for ch in text])\n",
    "dataset['review'] = dataset['review'].apply(is_special)\n",
    "dataset['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "dataset['review'] = dataset['review'].apply(str.lower)\n",
    "dataset['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "def rem_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [w for w in words if w not in stop_words]\n",
    "dataset['review'] = dataset['review'].apply(rem_stopwords)\n",
    "dataset['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stemming\n",
    "def stem_txt(text):\n",
    "    ss = SnowballStemmer('english')\n",
    "    return \" \".join([ss.stem(w) for w in text])\n",
    "dataset['review'] = dataset['review'].apply(stem_txt)\n",
    "dataset['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction using Bag of Words (BoW)\n",
    "We convert the cleaned text into a numeric matrix using CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=2000)\n",
    "X = cv.fit_transform(dataset['review']).toarray()\n",
    "y = dataset['sentiment'].values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split\n",
    "Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "Train three Naive Bayes models: Gaussian, Multinomial, and Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "mnb.fit(X_train, y_train)\n",
    "bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Trained Models\n",
    "Save all three models using `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gnb, 'MRSA_gnb.pkl')\n",
    "joblib.dump(mnb, 'MRSA_mnb.pkl')\n",
    "joblib.dump(bnb, 'MRSA_bnb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "Make predictions and evaluate the accuracy of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, gnb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, mnb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, bnb.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
